# -*- coding: utf-8 -*-
"""IDS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ph9lJhSNUzqC34Mcb7x1kPuIpWy1oeyi
"""

import zipfile

# Define the ZIP file path
zip_path = "/content/KDDTrain+.txt.zip"
extract_to = "/content/"

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print("Extraction complete!")

import zipfile

# Define the ZIP file path
zip_path = "/content/KDDTest+.txt.zip"
extract_to = "/content/"

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print("Extraction complete!")

!wget http://205.174.165.80/CICDataset/NSL-KDD/KDDTrain+.txt
!wget http://205.174.165.80/CICDataset/NSL-KDD/KDDTest+.txt

!pip install tensorflow scikit-learn

import pandas as pd

# Define dataset paths
train_file = "/content/KDDTrain+.txt"
test_file = "/content/KDDTest+.txt"

# Load dataset
train_data = pd.read_csv(train_file, header=None)
test_data = pd.read_csv(test_file, header=None)

# Print shapes
print("Train Data Shape:", train_data.shape)
print("Test Data Shape:", test_data.shape)

print("Number of columns in train_data:", len(train_data.columns))
print("Number of columns in test_data:", len(test_data.columns))

print("Number of columns in train_data:", len(train_data.columns))
print("Number of columns in test_data:", len(test_data.columns))

print(train_data.iloc[:, -5:].head())  # Check last 5 columns

train_data = train_data.iloc[:, :-1]  # Remove last column
test_data = test_data.iloc[:, :-1]

columns = ["duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
           "land", "wrong_fragment", "urgent", "hot", "num_failed_logins", "logged_in",
           "num_compromised", "root_shell", "su_attempted", "num_root", "num_file_creations",
           "num_shells", "num_access_files", "num_outbound_cmds", "is_host_login",
           "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
           "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
           "srv_diff_host_rate", "dst_host_count", "dst_host_srv_count",
           "dst_host_same_srv_rate", "dst_host_diff_srv_rate",
           "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate",
           "dst_host_serror_rate", "dst_host_srv_serror_rate",
           "dst_host_rerror_rate", "dst_host_srv_rerror_rate", "attack_type"]

# Assign column names
train_data.columns = columns
test_data.columns = columns

# Verify the first few rows
print(train_data.head())

from sklearn.preprocessing import LabelEncoder

# List of categorical columns to encode
categorical_cols = ["protocol_type", "service", "flag"]
label_encoders = {}

# Apply Label Encoding
for col in categorical_cols:
    le = LabelEncoder()
    train_data[col] = le.fit_transform(train_data[col])  # Encode training data
    test_data[col] = le.transform(test_data[col])  # Apply same encoding to test data
    label_encoders[col] = le  # Store encoder for later use

print("Categorical features encoded successfully!")

# Convert attack_type to binary labels
train_data["attack_type"] = (train_data["attack_type"] != 'normal').astype(int)
test_data["attack_type"] = (test_data["attack_type"] != 'normal').astype(int)

# Verify the changes
print("Binary encoding of attack_type completed!")
print(train_data["attack_type"].value_counts())  # Check class distribution

from sklearn.preprocessing import StandardScaler

# Select feature columns (excluding "attack_type")
feature_cols = train_data.columns[:-1]

# Initialize the scaler
scaler = StandardScaler()

# Apply scaling to training and test data
train_data[feature_cols] = scaler.fit_transform(train_data[feature_cols])
test_data[feature_cols] = scaler.transform(test_data[feature_cols])

print("Feature scaling completed!")

from sklearn.feature_selection import RFE
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Define features and labels
X = train_data.drop(columns=["attack_type"])
y = train_data["attack_type"]

# Feature Selection using RFE (Select Top 20 Features)
selector = RFE(RandomForestClassifier(n_estimators=50), n_features_to_select=20)
X_selected = selector.fit_transform(X, y)

# Dimensionality Reduction using PCA (Reduce to 10 Components)
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_selected)

print("Feature Selection & Dimensionality Reduction Completed!")
print(f"Original feature count: {X.shape[1]}, After RFE: {X_selected.shape[1]}, After PCA: {X_pca.shape[1]}")

from sklearn.model_selection import train_test_split

# Split the transformed features into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, stratify=y, random_state=42
)

# Display dataset shapes
print("Train-Test Split Completed!\n")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Check class distribution in training and testing sets
print("\nTraining Set Class Distribution:")
print(y_train.value_counts(normalize=True) * 100)

print("\nTesting Set Class Distribution:")
print(y_test.value_counts(normalize=True) * 100)

from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM

# Initialize Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
iso_forest.fit(X_train)

# Add anomaly scores to dataset
train_anomaly_scores = iso_forest.decision_function(X_train)
test_anomaly_scores = iso_forest.decision_function(X_test)

# Display results
print("Anomaly Detection Completed!\n")
print(f"Training Anomaly Scores (first 5 samples): {train_anomaly_scores[:5]}")
print(f"Testing Anomaly Scores (first 5 samples): {test_anomaly_scores[:5]}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

# Train SVM Classifier
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)
svm_pred = svm_model.predict(X_test)

# Display Accuracy Scores
print("Model Training Completed!\n")
print(f"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}")
print(f"SVM Accuracy: {accuracy_score(y_test, svm_pred):.4f}")

# Print Classification Reports
print("\nRandom Forest Classification Report:\n", classification_report(y_test, rf_pred))
print("\nSVM Classification Report:\n", classification_report(y_test, svm_pred))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding
from tensorflow.keras.optimizers import Adam

# Define DNN Model
dnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),  # Reduce overfitting
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

# Compile the model
dnn_model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

# Train the model
history = dnn_model.fit(X_train, y_train,
                        epochs=20,
                        batch_size=64,
                        validation_data=(X_test, y_test),
                        verbose=1)

# Evaluate the model
dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test, y_test)
print(f"DNN Test Accuracy: {dnn_accuracy:.4f}")

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Make predictions
y_pred_prob = dnn_model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype("int32")

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Normal", "Attack"], yticklabels=["Normal", "Attack"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - DNN Model")
plt.show()

# Compute ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f"ROC curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve - DNN")
plt.legend()
plt.show()

from sklearn.model_selection import cross_val_score
import numpy as np

# Perform 10-Fold Cross-Validation for Random Forest
rf_cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train, cv=5, scoring='accuracy')

# Perform 10-Fold Cross-Validation for SVM
svm_cv_scores = cross_val_score(SVC(kernel='rbf', probability=True, random_state=42), X_train, y_train, cv=5, scoring='accuracy')

# Display results
print("10-Fold Cross-Validation Completed!\n")
print(f"Random Forest Mean Accuracy: {np.mean(rf_cv_scores):.4f} ± {np.std(rf_cv_scores):.4f}")
print(f"SVM Mean Accuracy: {np.mean(svm_cv_scores):.4f} ± {np.std(svm_cv_scores):.4f}")

# Standalone Model Scores
rf_acc = accuracy_score(y_test, rf_pred)
svm_acc = accuracy_score(y_test, svm_pred)
dnn_acc = dnn_accuracy  # From previous training

# Hybrid Model (Averaging RF, SVM, and DNN Predictions)
hybrid_pred = (rf_pred + svm_pred + (dnn_model.predict(X_test) > 0.5).astype("int32").flatten()) / 3
hybrid_pred = (hybrid_pred > 0.5).astype("int32")
hybrid_acc = accuracy_score(y_test, hybrid_pred)

# Display results
print("Model Benchmarking Completed!\n")
print(f"SVM Accuracy: {svm_acc:.4f}")
print(f"DNN Accuracy: {dnn_acc:.4f}")
print(f"Random Forest Accuracy: {rf_acc:.4f}")
print(f"Hybrid IDS Accuracy: {hybrid_acc:.4f}")

from scipy.stats import ttest_rel

# Get prediction results for statistical testing
rf_preds = rf_pred.flatten()
svm_preds = svm_pred.flatten()
dnn_preds = (dnn_model.predict(X_test) > 0.5).astype("int32").flatten()
hybrid_preds = hybrid_pred.flatten()

# Perform paired t-tests
t_rf_hybrid, p_rf_hybrid = ttest_rel(rf_preds, hybrid_preds)
t_svm_hybrid, p_svm_hybrid = ttest_rel(svm_preds, hybrid_preds)
t_dnn_hybrid, p_dnn_hybrid = ttest_rel(dnn_preds, hybrid_preds)

# Display results
print("Statistical Validation (Paired t-tests) Completed!\n")
print(f"RF vs Hybrid IDS: t-value={t_rf_hybrid:.4f}, p-value={p_rf_hybrid:.4f}")
print(f"SVM vs Hybrid IDS: t-value={t_svm_hybrid:.4f}, p-value={p_svm_hybrid:.4f}")
print(f"DNN vs Hybrid IDS: t-value={t_dnn_hybrid:.4f}, p-value={p_dnn_hybrid:.4f}")

# Interpretation
if p_rf_hybrid < 0.05 and p_svm_hybrid < 0.05 and p_dnn_hybrid < 0.05:
    print("\nThe Hybrid IDS model is statistically significantly better than standalone models! 🚀")
else:
    print("\nNo significant improvement. Consider refining feature selection or adding more data.")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Optimized DNN Model with Fewer Layers
dnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Reduced neurons
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # Output layer
])

# Compile the model with a higher learning rate for faster training
dnn_model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

# Train the model with fewer epochs and larger batch size
history = dnn_model.fit(X_train, y_train,
                        epochs=10,  # Reduced from 30 to 10
                        batch_size=256,  # Increased batch size for faster training
                        validation_data=(X_test, y_test),
                        verbose=1)

# Evaluate the model
dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test, y_test)
print(f"Faster DNN Training Complete! Test Accuracy: {dnn_accuracy:.4f}")

import matplotlib.pyplot as plt

# Plot Training vs. Validation Accuracy
plt.figure(figsize=(6, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='green')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Training vs. Testing Accuracy - Overfitting Check")
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Plot Training vs. Validation Loss
plt.figure(figsize=(6, 5))
plt.plot(history.history['loss'], label='Training Loss', color='red')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs. Testing Loss - Overfitting Check")
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Train SVM
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# Define & Train DNN
dnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

dnn_model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

dnn_model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1)

import numpy as np

# Get predictions (binary classification)
rf_pred = rf_model.predict(X_test)
svm_pred = svm_model.predict(X_test)
dnn_pred = (dnn_model.predict(X_test) > 0.5).astype("int32").flatten()  # Convert sigmoid output to binary

# Get probability scores (for ROC Curve)
rf_proba = rf_model.predict_proba(X_test)[:, 1]
svm_proba = svm_model.predict_proba(X_test)[:, 1]
dnn_proba = dnn_model.predict(X_test).flatten()

# Majority Voting: If 2 out of 3 models agree, we take that as the final prediction
hybrid_pred = (rf_pred + svm_pred + dnn_pred) / 3
hybrid_pred = (hybrid_pred > 0.5).astype("int32")  # Convert to binary class

# Probability-based Hybrid Prediction (for ROC Curve)
hybrid_proba = (rf_proba + svm_proba + dnn_proba) / 3  # Average probabilities

from sklearn.metrics import accuracy_score

# Compute accuracy
rf_acc = accuracy_score(y_test, rf_pred)
svm_acc = accuracy_score(y_test, svm_pred)
dnn_acc = accuracy_score(y_test, dnn_pred)
hybrid_acc = accuracy_score(y_test, hybrid_pred)

# Print results
print(f"Random Forest Accuracy: {rf_acc:.4f}")
print(f"SVM Accuracy: {svm_acc:.4f}")
print(f"DNN Accuracy: {dnn_acc:.4f}")
print(f"Hybrid IDS Accuracy: {hybrid_acc:.4f}")  # This should ideally be the highest

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

models = {"Random Forest": rf_pred, "SVM": svm_pred, "DNN": dnn_pred, "Hybrid IDS": hybrid_pred}
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

for i, (name, preds) in enumerate(models.items()):
    row, col = i // 2, i % 2  # Position in grid
    cm = confusion_matrix(y_test, preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[row, col])
    axes[row, col].set_title(f'{name} - Confusion Matrix')
    axes[row, col].set_xlabel('Predicted')
    axes[row, col].set_ylabel('Actual')

plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc

# Define models and probabilities
proba_models = {
    "Random Forest": rf_proba,
    "SVM": svm_proba,
    "DNN": dnn_proba,
    "Hybrid IDS": hybrid_proba
}

plt.figure(figsize=(8, 6))

for name, y_proba in proba_models.items():
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'r--')  # Random guess line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - All Models')
plt.legend()
plt.show()

!pip install shap lime

!pip install shap lime
import shap
import lime
import lime.lime_tabular
import numpy as np
import matplotlib.pyplot as plt

# Initialize SHAP Explainer (Use Random Forest for interpretation)
explainer_shap = shap.Explainer(rf_model.predict, X_train)

# Compute SHAP values (use a subset for efficiency)
shap_values = explainer_shap(X_test[:50])

# Display SHAP Summary Plot
shap.summary_plot(shap_values, X_test[:50], feature_names=X.columns)

# If you loaded data from a Pandas DataFrame
feature_names = list(train_data.drop(columns=["attack_type"]).columns)  # Ensure 'train_data' is defined

print(f"X_train shape: {X_train.shape}")  # Verify the number of features

# If RFE was used:
selected_features = X.columns[selector.support_]  # Only selected features
feature_names = list(selected_features)

# If PCA was used:
feature_names = [f'PC{i+1}' for i in range(X_train.shape[1])]  # Rename as PC1, PC2, ..., PC10

import pandas as pd

# Ensure X_train and X_test are DataFrames with matching columns
X_train = pd.DataFrame(X_train, columns=feature_names)
X_test = pd.DataFrame(X_test, columns=feature_names)

print(f"New X_train shape: {X_train.shape}")  # Verify correction

import lime.lime_tabular

# Initialize LIME Explainer with the corrected feature names
explainer_lime = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Normal', 'Attack'],
    mode='classification'
)

# Select a test sample for explanation
idx = 5
exp = explainer_lime.explain_instance(X_test.iloc[idx].values, rf_model.predict_proba)

# Show the explanation
exp.show_in_notebook()

!pip install shap

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

import pandas as pd
if isinstance(X_test, np.ndarray):
    X_test = pd.DataFrame(X_test, columns=X_train.columns)

pred = rf_model.predict_proba(X_test[:500])
print("Predictions sum check:", pred.sum(axis=1))

X_train = X_train.to_numpy()  # Convert before model training
X_test = X_test.to_numpy()    # Convert before SHAP explanation

print("X_train columns:", X_train.columns if hasattr(X_train, "columns") else "No feature names")
print("X_test columns:", X_test.columns if hasattr(X_test, "columns") else "No feature names")

print(type(X_train), type(X_test))

shap_values = explainer_shap(X_test[:500])

explainer_shap = shap.TreeExplainer(rf_model)  # Or your respective model
shap_values = explainer_shap.shap_values(X_test[:500])

print(np.array(shap_values).shape)

print(f"SHAP values shape: {shap_values[1].shape}")  # Should be (500, 10)
print(f"X_test[:500] shape: {X_test[:500].shape}")  # Should be (500, 10)

shap_values_corrected = shap_values[:, :, 1]  # Use correct class index
shap.summary_plot(shap_values_corrected, X_test[:500], feature_names=feature_names)

!pip install shap matplotlib scikit-learn tensorflow
import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

shap_values_rf = explainer_rf(X_test[:500], check_additivity=False)

import shap

# Ensure X_test is a DataFrame for SHAP
import pandas as pd
X_test_df = pd.DataFrame(X_test, columns=feature_names)

# Check if expected_value is an array (some models return a list)
expected_value_svm = explainer_svm.expected_value
if isinstance(expected_value_svm, (list, np.ndarray)):
    expected_value_svm = expected_value_svm[0]  # Take the first element if it's a list

# Generate force plot
shap.force_plot(expected_value_svm, shap_values_svm[0], X_test_df.iloc[0], matplotlib=True)

import shap

# Use a small subset for SHAP calculations
X_sample = shap.sample(X_train, 100)  # Reducing to 100 samples

# Fast TreeExplainer for RF
explainer_rf = shap.TreeExplainer(rf_model)
shap_values_rf = explainer_rf.shap_values(X_sample)

# Summary Plot
shap.summary_plot(shap_values_rf, X_sample)

import shap

# Reduce training size
X_sample = shap.sample(X_train, 50)  # Reduce to 50 samples

# Approximate KernelExplainer for SVM
explainer_svm = shap.KernelExplainer(svm_model.predict, X_sample)
shap_values_svm = explainer_svm.shap_values(X_test[:50])  # Only 50 test samples

# Summary Plot
shap.summary_plot(shap_values_svm, X_test[:50])

!pip install lime

import lime
import lime.lime_tabular

print(type(X_train))  # Check data type
print(X_train.shape)  # Check if it has data

explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train,   # If already a NumPy array
    feature_names=feature_names,
    class_names=["Normal", "Attack"],
    mode="classification"
)

X_train_df = pd.DataFrame(X_train, columns=feature_names)

explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train_df.to_numpy(),
    feature_names=X_train_df.columns,
    class_names=["Normal", "Attack"],
    mode="classification"
)

import lime
import lime.lime_tabular
import numpy as np

# Ensure X_train_df is defined (Convert to DataFrame if needed)
X_train_df = pd.DataFrame(X_train, columns=feature_names)
X_test_df = pd.DataFrame(X_test, columns=feature_names)

# Define LIME Explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train_df.to_numpy(),
    feature_names=X_train_df.columns,
    class_names=["Normal", "Attack"],
    mode="classification"
)

# Select an instance from test data
test_instance = X_test_df.iloc[0].to_numpy()

# Explain prediction for Random Forest
exp_rf = explainer.explain_instance(test_instance, rf_model.predict_proba)
exp_rf.show_in_notebook()

# Explain prediction for SVM
exp_svm = explainer.explain_instance(test_instance, svm_model.predict_proba)
exp_svm.show_in_notebook()